### 什么是卷积神经网络

```Python
#MNIST 使用卷积神经网络进行分类
from keras import models
from keras import layers
from keras.datasets import mnist
from keras.utils import to_categorical


(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape(60000, 28, 28, 1)
train_images = train_images.astype('float32') / 255
test_images = test_images.reshape(10000, 28, 28, 1)
test_images = test_images.astype('float32') / 255
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

model = models.Sequential()
model.add(layers.Conv2D(32, (2, 2), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (2, 2), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (2, 2), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=["accuracy"])
model.fit(train_images, train_labels, epochs=5, batch_size=64)

test_loss, test_acc = model.evaluate(test_images, test_labels)
print(test_acc)
```

- 卷积运算

  >  Dense和Conv2D的区别在于，Dense从输入特征空间中学到的是全局模式， 而Conv2D学到的是局部模式(卷积核的作用)，所以卷积神经网络有以下两个有趣的性质：
  >
  > - 卷积神经网络学到的模式具有平移不变性，即从训练集中学到的模式，换成任何图像都可以识别这个模式，而Dense层是整体学习，原本模式的一些细节都会引起变化，Conv2D学习的就是细节
  > - 卷积神经网络可以学到模式的空间层次结构，每一层的卷积层都会在上一层的基础上学习到更宽泛的模式，比如第一层学习的是原子级的细节，第二层则会学习到分子级细节，以此类推，到最后一层可能就会学习到更具象化的模式

- 卷积由下面两个关键参数定义

  > - 从输入中提取的图块尺寸，即卷积核的大小，通常是3x3或者5x5
  >
  > - 输出特征图的深度，卷积所计算的的过滤器数量，就是Conv2D的第一个参数

- 卷积的工作原理：

  > 在3D输入特征图上滑动(滑动的窗口就是卷积核)，在每个可能的位置停止并提取周围特征的3D图块，形状为(卷积核h, 卷积核w, 输入特征图的depth)，然后再每个3D图块与学到的同一个权重矩阵(卷积核)做张量积运算，转换成(output_depth, )的ID向量，然后对所有的这些向量进行空间重组，使其转换为(h, w, output_depth)的3D输出特征图。输出特征图中的每个空间位置都对应输入特种图中的相同位置。
  >
  > TIPS：
  >
  > - 输入的宽高和输出的宽高可能不同，
  >   - *边界效应*：窗口和长宽的关系， Conv2D中可以指定padding参数(valid, same)(不填充，填充后输入与输出的长宽相同)
  >   - *使用了步幅(stride)*

- 最大池化运算

  > 对特种图进行下采样，与步进卷进类似，最大池化使用硬编码的max张量运算对局部图块进行变换，而不是使用学到的线性变换(卷积核)。**使用下采样的原因，一是减少需要处理的特征图的元素个数，二是通过让连续卷积层的观察窗口越来越大，从而引入空间过滤器的层级结构。**
  >
  > 而不使用池化运算的情况下，不利于学习特征的空间层级结构，下一层学习到的特征总是基于上一层，指挥越来越小，而且最后一层的特征图的元素(h * w * depth)数量会非常多，若将其展平在一个512大小的Dense层上，参数会有 512 * h * w * depth 个，对于小型模型来说，会导致严重的过拟合


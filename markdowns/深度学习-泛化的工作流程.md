### 如何整合为七步工作流程，解决学习任务

- 机器学习的四个分支
  - 监督学习
    - 序列生成
    - 语法树检测
    - 目标检测
    - 图像分割
  - 无监督学习：数据分析的必备技能
    - 降维
    - 聚类
  - 自监督学习
    - 自编码器
  - 强化学习

- 如何评估机器学习模型，衡量泛化能力

  - 包含三个集合的数据集：训练集，验证集，测试集

    - 通过在训练数据上训练模型，在验证集上评估模型，从而找到最佳参数，最后在测试集上验证测试。

  - 对于数据量不够的情况，可以使用三种经典的评估方法：

    - 简单的留出验证：

      - 留出一定比例的数据作为测试集和验证集，在剩余数据上训练模型

      - ```Python
        num_validation_samples = 10000
        np.random.shuffle(data) # 打乱数据
        validation_data = data[:num_validation_samples]
        data = data[num_validation_sample:]
        training_data = data[:]
        
        model = get_model()
        model.train(training_data)
        validation_score = model.evaluate(validation_data)
        
        # 根据数据进行模型调节，重新训练，评估，然后再次调节...
        
        # 最后,在测试集上进行评估
        model = get_model()
        model.train(np.concatenate([training_data, 			    							validation_data]))
        test_score = model.evaluate(test_data)
        ```

    - K折交叉验证：

      - 将数据划分为大小相同的K个分区，对每个分区，在0->K-1上进行训练，并在最后一个数据集上进行验证，最终分数为K个分数的平均值

      - ```Python
        k = 4
        num_validation_samples = len(data) // k
        np.random.shuffle(data)
        validation_scores = []
        for fold in range(K):
            validation_data = data[num_validation_samples*fold: num_validation_sapmles * (fold + 1)]
            training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1):]
            model = get_model()
            model.train(training_data)
            validation_score = model.evaluate(validation_data)
            validation_scores.append(validation_score)
        # 验证分数
        validation_score = np.average(validation_scores)
        
        # 验证调整后得到最终的参数
        model = get_model()
        model.train(data)
        test_score = model.evaluate(test_data)
        ```

    - 有打乱顺序的重复K折验证

      - 每次K折验证前都打乱数据，计算代价很大

- 数据预处理、特征工程和特征学习
  - 在数据输入神经网络之前，要将数据进行预处理，从而减少因为数据不均衡导致的训练问题
  - 神经网络的数据预处理
    - 向量化
      - 神经网络的所有输入和目标都必须是浮点数张量，特定情况下可以是整数张量，无论处理什么数据(声音，图像，文本)，都必须首相将其转换为张量，这一步叫做数据向量化
    - 值标准化
      - 需要对每个特征分别做标准化，使其均值为0， 标准差为1
      - 输入数据的特征：
        - 取值较小，大部分值要在0-1之间
        - 同质性：所有特征的取值都应该在大致相同的范围
    - 处理缺失值
      - 如果0不是一个有意的值，则通常将数据中的缺失值设为0
  - 特征工程
    - 将原始数据提取出具有学习意义的特征，可以方便神经网络的学习，和减少工作量

- 过拟合与欠拟合

  - 欠拟合是网络训练不够

  - 过拟合则是模型学习仅和训练数据有关的模式，从而缺乏对新数据的作用能力

  - 欠拟合做优的解决办法就是增加训练数据，次优解决办法是调节模型允许存储的信息量，或对模型允许存储的信息加以约束，这样网络在对约束内的模式则会有良好的泛化，不过局限性就会变大

  - 正则化：以上降低过拟合的方法称作正则化，几种常见的正则化方法：

    - 减少网格大小， 减少模型大小，较少模型中可学习参数的个数(这是由层数和每层的单元个数决定的)

    - 权重正则化，用来是模型权重只能取较小的值，从而限制模型的复杂度，是权重的值分布更加规则

      - L1 正则化：添加的成本与权重系数的绝对值成正比

      - L2 正则化：添加的成本与权重系数的平方成正比，L2正则化也称作权重衰减

      - ```Python
        from keras import regularizers, models, layers
        model = models.Sequential()
        model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,)))
        model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'))
        model.add(layers.Dense(1, activation="sigmoid"))
        ```

    - dropout 正则化

      - 丢弃一部分的输出特征，最有效也最常用的方法之一

      - ```Python
        model.add(layers.Dropout(rate)) # rate 置零比例
        ```

#### 通用工作流程

- **定义问题，收集数据集**

  - 输入数据是什么？预测什么？
  - 面对的是什么类型的问题？二分类，多分类，标量回归？帮助选择模型架构，损失函数等

- **选择衡量成功的指标**

- **确定评估方法**

  - 留出验证集
  - K折交叉验证
  - 重复的K折验证

- **准备数据**

  - 将数据格式化为张量
  - 这些张量的取值通常应该缩放为较小的值，比如在[-1, 1]、[0, 1]之间
  - 如果不同的特征具有不同的取值范围，那么应该做数据标准化
  - 尤其对于小数据问题，需要做特征工程

- **开发比基准更好的模型**

  - 这一阶段目标是获得统计功效，即开发一个小型模型，能够打败纯随机的基准，比如在MNIST中任何 精度大于0.1的模型都可以说具有 统计功效，最后需要选择三个关键参数来构建第一个工作模型

    - 最后一层的激活，即根据输入选择相应的激活，IMDB最后一层使用了sigmoid，回归的例子最后一层没有使用激活等。

    - 损失函数：应该要匹配你要解决问题的类型。例如，IMDB使用binary_crossentropy，回归使用mse等

    - 优化配置: 使用哪种优化器，学习率是多少，大多数情况下，rmsprop及其默认学习率是最稳妥的

    - | 问题类型            | 最后一层激活 | 损失函数                 |
      | ------------------- | ------------ | ------------------------ |
      | 二分类              | sigmoid      | binary_crossentropy      |
      | 多分类、单标签      | softmax      | categorical_crossentropy |
      | 多分类、多标签      | sigmoid      | binary_crossentropy      |
      | 回归到任意值        | 无           | mse                      |
      | 回归到0-1范围内的值 | sigmoid      | mse/binary_crossentropy  |

- **扩大模型规模：开发过拟合的模型**

  - 添加更多的层
  - 让每一层变大
  - 训练更多的轮次

- **模型正则化与调节超参数**

  - 添加dropout
  - 尝试不同的架构
  - 添加L1和/或者L2正则化
  - 尝试不同的超参数(比如每层的单元个数，或优化器的学习率)
  - 反复做特征工程：添加新特征或者删除没有信息量的特征
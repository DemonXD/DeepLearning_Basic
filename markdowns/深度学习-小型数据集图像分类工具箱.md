### 使用kaggle的猫狗分类数据集，泛化一个工具箱

**工具集包含猫狗各12500张训练数据集，和各6250张测试数据集，并且图片尺寸不统一**

- 使用小数据集进行训练和测试

  - 训练数据集：猫：1000， 狗：1000
  - 验证数据集：猫：500， 狗：500
  - 测试数据集：猫：500， 狗：500

- 问题分析：

  - 首先这个网络最后是执行一个二分类问题，所以最后一层是用sigmoid激活函数

  - 输入尺寸需要进行统一，这里选择150x150x3

  - 由于问题比较复杂，所以网络相比MNIST要增加几层，这里选用：

    > Conv2D(32, (3, 3))
    >
    > MaxPooling2D((2, 2))
    >
    > Conv2D(64, (3, 3))
    >
    > MaxPooling2D((2, 2))
    >
    > Conv2D(128, (3, 3))
    >
    > MaxPooling2D((2, 2))
    >
    > Conv2D(128, (3, 3))
    >
    > MaxPooling2D((2, 2))
    >
    > Flatten()
    >
    > Dense(512)
    >
    > Dense(1, "sigmoid")
    >
    > 建模后使用model.summary()，可以看到最后一层池化结束时，特征图的尺寸为7x7x128，
    >
    > 展平后送到512大小的Dense层，总的参数数量达到了3211776

  - 模型编译时，选用RMSprop优化器，learning_rate设置为1e-4(0.0001)

  - 整个基于keras的模型如下：

    > ```Python
    > model = models.Sequential()
    > model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
    > model.add(layers.MaxPooling2D((2, 2)))
    > model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    > model.add(layers.MaxPooling2D((2, 2)))
    > model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    > model.add(layers.MaxPooling2D((2, 2)))
    > model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    > model.add(layers.MaxPooling2D((2, 2)))
    > model.add(layers.Flatten())
    > model.add(layers.Dense(512, activation='relu'))
    > model.add(layers.Dense(1, activation="sigmoid"))
    > 
    > model.compile(
    >     optimizer=optimizers.RMSprop(lr=0.0001),
    >     loss="binary_crossentropy",
    >     metrics=["accuracy"]
    > )
    > ```

- 数据预处理：

  - 读取图像文件
  - 将JEPG文件解码为RGB像素网格
  - 将像素网格转换为浮点数张量
  - 将像素值(0~255)缩放到[0~1]区间，让神经网络处理较小的输入值可以减轻压力

  > keras提供了图片处理的工具，keras.preprocessing.image
  >
  > ```Python
  > from keras.preprocessing.image import ImageDataGenerator
  > train_datagen = ImageDataGenerator(rescale=1./255) # 将所有图像乘以1/255
  > train_generator = train_datagen.flow_from_directory(
  >     train_dir,						# 图像地址
  >     target_size=(150, 150),			# 目标大小
  >     batch_size=20,					
  >     class_mode='binary'				# 因为使用binary_crossentropy。所以使用二进制标签
  > )
  > ```

- 训练：

  > ```PYthon
  > history = model.fit_generator(
  >     train_generator,
  >     steps_per_epoch=100,
  >     epochs=30,
  >     validation_data=validation_generator,
  >     validation_steps=50
  > )
  > # 根据训练完成的数据，可以得知，验证精度到达73左右，就不再升高了，而训练精度却一直在增加，并且验证损失，在5个epochs后，就开始保持不变了，而训练损失一直在降低，由此可知，训练发生了过拟合
  > ```
  - 由于数据量少，造成了训练的过拟合，此处使用一种对于小数据集的增强方法，数据增强，其作用是利用多种能够生成可信图像的随即变换来增加训练样本，在keras中ImageDataGenerator就有这个功能，通过调整其部分参数就可以达到要求：

  > ```Python
  > # 数据增强
  > datagen = ImageDataGenerator(
  >     rotation_range=40,           # 图像随机旋转的角度范围(0~180)
  >     width_shift_range=0.2,       # 图像在水平方向上平移的范围（比例）
  >     height_shift_range=0.2,      # 图像在垂直方向上平移的范围（比例）
  >     shear_range=0.2,             # 随机错切变换的角度
  >     zoom_range=0.2,              # 图像随机缩放的范围
  >     horizontal_flip=True,        # 随机将一半图像水平翻转
  >     fill_mode="nearest"          # 填充新创建像素的方法
  > )
  > ```
  - 虽然使用数据增强可以帮助缓解过拟合，但是无法完全消除，所以在Dense(512)之前，加一层Dropout层，进行一次随机丢弃

  - 此时新的网络：

    > ```python
    > model = models.Sequential()
    > model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
    > model.add(layers.MaxPooling2D((2, 2)))
    > model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    > model.add(layers.MaxPooling2D((2, 2)))
    > model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    > model.add(layers.MaxPooling2D((2, 2)))
    > model.add(layers.Conv2D(128, (3, 3), activation='relu'))
    > model.add(layers.MaxPooling2D((2, 2)))
    > model.add(layers.Flatten())
    > model.add(layers.Dropout(0.5))
    > model.add(layers.Dense(512, activation='relu'))
    > model.add(layers.Dense(1, activation="sigmoid"))
    > 
    > model.compile(
    >     optimizer=optimizers.RMSprop(lr=0.0001),
    >     loss="binary_crossentropy",
    >     metrics=["acc"]
    > )
    > ```

  - 对输入数据进行数据增强：

    > ```Python
    > train_datagen = ImageDataGenerator(
    >     rescale=1./255,
    >     rotation_range=40,           # 图像随机旋转的角度范围(0~180)
    >     width_shift_range=0.2,       # 图像在水平方向上平移的范围（比例）
    >     height_shift_range=0.2,      # 图像在垂直方向上平移的范围（比例）
    >     shear_range=0.2,             # 随机错切变换的角度
    >     zoom_range=0.2,              # 图像随机缩放的范围
    >     horizontal_flip=True,        # 随机将一半图像水平翻转
    >     fill_mode="nearest"          # 填充新创建像素的方法
    > )
    > train_generator = train_datagen.flow_from_directory(
    >     train_dir,
    >     target_size=(150, 150),
    >     batch_size=32,
    >     class_mode='binary')
    > history = model.fit_generator(
    >     train_generator,
    >     steps_per_epoch=100,
    >     epochs=100,
    >     validation_data=validation_generator,
    >     validation_steps=50
    > )
    > ```
    >
    > 

- 使用数据增强和dropout，可以在一定程度上解决过拟合的问题，但是如果想要再进一步提升网络的精度，除了使用无限量的训练集，还有一种办法就是使用**预训练网络**， 预训练网络是一个保存好的网络，之前在大型数据集上训练好。使用预训练网络有两种方法：**特征提取**和**微调模型**。

  - **特征提取**：

    - 使用之前昂罗学到的表示来从新样本中提取出有趣的特征，然后将这些特征输入一个新的分类器，从头开始训练。
    - 图像分类的卷积神经网络包含两部分，首先是一些列池化层和卷积层，然后是一个密集连接分类器，第一部分叫做模型的**卷积基**，对于卷积神经网络而言，特征提取就是去除之前训练好的网络的卷积基，在上面运行新数据，然后在输出上面训练一个新的分类器。

    > keras 中内置了一些之前网络的模型：VGG16，VGG19等。
    >
    > ```Python
    > from keras.applications import VGG16
    > conv_base = VGG16(
    > 	weights='imagenet', 		# 指定模型初始化的权重检查点
    >     include_top=False, 			# 是否需要密集连接分类器，如果是自定义的分类器，这里选择False
    >     input_shape=(150, 150, 3)	# 指定输入数据的尺寸
    > )
    > 
    > # 查看网络层级
    > conv_base.summary()
    > ```

    - 添加自定义密集分类器：

      - 在自己的数据集上运行卷积基，将输出保存成硬盘中的Numpy数组，然后用这个数据作为输入，输入到独立的密集连接分类器中，这种方法速度快，计算代价低，因为对于每个输入图像只需要运行一次卷积基，而卷积基是目前流程中计算代价最高的，但是使用这种方法，无法使用数据增强

        > ```python
        > # 处理原始数据
        > base_dir = r"D:\Practice_Code\Python\DEEP_LEARNING\toolsets\small_pictures"
        > train_dir = os.path.join(base_dir, "train")
        > validation_dir = os.path.join(base_dir, "validation")
        > test_dir = os.path.join(base_dir, "test")
        > 
        > datagen = ImageDataGenerator(
        >     rescale=1./255
        > )
        > batch_size = 20
        > def extract_features(directory, sample_count):
        >     features = np.zeros(shape=(sample_count, 4, 4, 512))
        >     labels = np.zeros(shape=(sample_count))
        >     generator = datagen.flow_from_directory(
        >         directory,
        >         target_size=(150, 150),
        >         batch_size=batch_size,
        >         class_mode="binary"
        >     )
        >     i = 0
        >     for inputs_batch, labels_batch in generator:
        >         features_batch = conv_base.predict(inputs_batch)
        >         features[i * batch_size : (i+1) * batch_size] = features_batch
        >         labels[i*batch_size: (i+1)*batch_size] = labels_batch
        >         i+=1
        >         if i*batch_size >= sample_count:
        >             break
        >     return features, labels
        > 
        > train_features, train_labels = extract_features(train_dir, 2000)
        > validation_features, validation_labels = extract_features(validation_dir, 1000)
        > test_features, test_labels = extract_features(test_dir, 1000)
        > 
        > # 展平后，可以将数据塞入密集连接分类器中
        > train_features = np.reshape(train_features, (2000, 4 * 4 * 512))
        > validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))
        > test_features = np.reshape(test_features, (1000, 4 * 4 * 512))
        > 
        > from keras import models, layers, optimizers
        > 
        > model = models.Sequential()
        > model.add(layers.Dense(256, activation='relu', input_dim=(4 * 4 * 512)))
        > model.add(layers.Dropout(0.5))
        > model.add(layers.Dense(1, activation="sigmoid"))
        > 
        > model.compile(
        >     optimizer=optimizers.RMSprop(lr=2e-5),
        >     loss="binary_crossentropy",
        >     metrics=["acc"]
        > )
        > history = model.fit(
        >     train_features, train_labels,
        >     epochs=30, batch_size=20,
        >     validation_data=(validation_features, validation_labels))
        > ```

      - 在顶部添加Dense层来扩展，并在输入数据上端到端的运行整个模型，这样可以使用数据增强，但是这种方法的计算代价太大
      
        > ```python
        > onv_base = VGG16(
        >     weights='imagenet',
        >     include_top=False,
        >     input_shape=(150, 150, 3)
        > )
        > conv_base.trainable = False
        > model = models.Sequential()
        > model.add(conv_base)
        > model.add(layers.Flatten())
        > model.add(layers.Dense(256, activation='relu'))
        > model.add(layers.Dense(1, activation="sigmoid"))
        > 
        > model.compile(
        >     optimizer=optimizers.RMSprop(lr=2e-5),
        >     loss="binary_crossentropy",
        >     metrics=["acc"]
        > )
        > # 输入的数据集格式
        > # 数据预处理
        > train_dir = r"D:\Practice_Code\Python\DEEP_LEARNING\toolsets\small_pictures\train"
        > validation_dir = r"D:\Practice_Code\Python\DEEP_LEARNING\toolsets\small_pictures\validation"
        > train_datagen = ImageDataGenerator(
        >     rescale=1./255,
        >     rotation_range=40,           # 图像随机旋转的角度范围(0~180)
        >     width_shift_range=0.2,       # 图像在水平方向上平移的范围（比例）
        >     height_shift_range=0.2,      # 图像在垂直方向上平移的范围（比例）
        >     shear_range=0.2,             # 随机错切变换的角度
        >     zoom_range=0.2,              # 图像随机缩放的范围
        >     horizontal_flip=True,        # 随机将一半图像水平翻转
        >     fill_mode="nearest"          # 填充新创建像素的方法
        > )
        > 
        > #不能增强验证数据
        > test_datagen = ImageDataGenerator(rescale=1./255)
        > 
        > train_generator = train_datagen.flow_from_directory(
        >     train_dir,
        >     target_size=(150, 150),
        >     batch_size=20,
        >     class_mode='binary')
        > validation_generator = test_datagen.flow_from_directory(
        >     validation_dir,
        >     target_size=(150, 150),
        >     batch_size=20,
        >     class_mode='binary')
        > 
        > # 训练
        > history = model.fit_generator(
        >     train_generator,
        >     steps_per_epoch=100,
        >     epochs=30,
        >     validation_data=(validation_generator),
        >     validation_steps=50
        > )
        > ```

  - **微调模型**

    - 只冻结部分的几层，进行网络训练

      > - 在已训练好的基网络上添加自定义网络
      > - 冻结基网络
      > - 训练所添加的部分
      > - 解冻基网络的一些层
      > - 联合训练解冻的这些层和添加的部分
      >
      > ```Python
      > # 1. 在已训练的基网络上添加自定义网络
      > # 2. 冻结基网络
      > # 3. 训练添加部分
      > # 前三步的代码在上一部分已经实现
      > # 4. 解冻基网络的一些层
      > conv_base.trainable = True
      > set_trainable = False
      > for layer in conv_base.layers:
      >     # 考虑微调部分参数，这里只微调后三层网络
      >     if layer.name == 'block5_conv1': # block5_conv1 之后的层都解冻
      >         set_trainable = True
      >     if set_trainable:
      >         layer.trainable = True
      >     else:
      >         layer.trainable = False
      > # 编译
      > model.compile(
      > 	loss='binary_crossentropy',
      >     # 这里使用更小的学习率进行训练,更小的学习率权重更新不会太大
      >     # 太大的权重有可能会破快该层原本的表现
      >     optimizer=optimizers.RMSprop(lr=1e-5),
      >     matrics=['acc']
      > )
      > # 5. 联合训练
      > history = model.fit_generator(
      > 	train_generator,
      >     steps_per_epoch=100,
      >     epochs=100,
      >     validation_data=validation_generator,
      >     validation_steps=50
      > )
      > ```
      >
      > 

      > Tips:
      >
      > 有时最后的数据用plot打印出来会有比较波动大的锯齿，可能存在一些噪声，所以为了图像的可读性，可以将每个损失和精度替换成**指数移动平均值**，从而让曲线变得平滑。
      >
      > ```Python
      > # 简单实现
      > def smooth_curve(points, factor=0.8):
      >     smoothed_points = []
      >     for point in points:
      >         if smoothed_points:
      >             previous = smoothed_points[-1]
      >             smoothed_points.append(previous * factor + point * (1 - factor))
      >         else:
      >             smoothed_points.append(point)
      >     return smoothed_points
      > ```

  - 验证精度：

    > ```python
    > test_generator = test_datagen.flow_from_directory(
    > 	test_dir,
    >     target_size=(150, 150),
    >     batch_size=20,
    >     class_mode=['binary']
    > )
    > 
    > test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)
    > print(test_acc)
    > ```
    >
    > 

### 总结

- 卷积神经网络是用于计算机视觉任务的最佳机器学习模型，即使在非常小的数据集上也可以从头开始训练一个结果还不错的卷积神经网络
- 在小型数据集上的主要问题是过拟合，在处理图像数据时，数据增强时一种降低过拟合的强大方法，还可以适当的搭配，dropout和L2
- 利用特征提取，可以很容易将现有的卷积神经网络复用于新的数据集，对小型图像数据集，这是一种很有价值的方法
- 作为特征 提取的补充，可以使用微调，将现有模型之前学到的一些数据表示应用于新的问题，这种方法可以进一步提高模型性能